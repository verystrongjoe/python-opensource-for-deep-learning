{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 오늘은 4일차\n",
    "\n",
    "\n",
    "## 3일차 복습\n",
    "\n",
    "PCA를 통해서 차원축소를 하는데 과적합과 과소적합의 중간을 결정\n",
    "\n",
    "train자체는 w,b를 조절하는 최적화를 통해서 train의 score를 optimize하고\n",
    "train과 validation의 차이가 심하면 과적합이고 . \n",
    "모의고사 좀 풀고 수능을 잘한다고 확신한다는 느낌이라고 함 -> 샘플 수를 늘려서 일반화( 샘플수를 늘려서)\n",
    "\n",
    "\n",
    "정확도.\n",
    "learning_curve sample을 tunign해서 과적합 조정 \n",
    "validation_curve로 hyper parameter를 tuning했고 마찬가지로 과적합 조정\n",
    "\n",
    "\n",
    "GridSearch로 \n",
    "\n",
    "\n",
    "Confusion Matrix로 다양한 지표를 통해 모델 검증 Roc Curve로 봄\n",
    "\n",
    "1) 최적화\n",
    "\n",
    "2) 일반화\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##  오늘은 4일차\n",
    "\n",
    "### 다층 퍼셉트론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def 로직게이트생성(w,b)  :\n",
    "    def logic_gate(x1,x2) :\n",
    "        x = np.array([x1,x2])\n",
    "        z = np.sum(w*x) + b\n",
    "        y = 1 if z > 0 else 0\n",
    "        return y\n",
    "    return logic_gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(logic) :\n",
    "    for x1, x2 in [(0, 0), (0, 1), (1, 0), (1, 1)]:\n",
    "        y = logic(x1, x2)\n",
    "        print(x1, x2, '|', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AND = 로직게이트생성(np.array([0.5, 0.5]), -0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 | 0\n",
      "0 1 | 0\n",
      "1 0 | 0\n",
      "1 1 | 1\n"
     ]
    }
   ],
   "source": [
    "test(AND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NAND = 로직게이트생성(np.array([-0.5, -0.5]), 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 | 1\n",
      "0 1 | 1\n",
      "1 0 | 1\n",
      "1 1 | 0\n"
     ]
    }
   ],
   "source": [
    "test(NAND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OR = 로직게이트생성(np.array([0.5, 0.5]), -0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 | 0\n",
      "0 1 | 1\n",
      "1 0 | 1\n",
      "1 1 | 1\n"
     ]
    }
   ],
   "source": [
    "test(OR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다층 퍼셉트론으로 XOR해결하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def XOR(x1,x2) :\n",
    "    s1 = NAND(x1,x2)\n",
    "    s2 = OR(x1,x2)\n",
    "    y = AND(s1,s2)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 | 0\n",
      "0 1 | 1\n",
      "1 0 | 1\n",
      "1 1 | 0\n"
     ]
    }
   ],
   "source": [
    "test(XOR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 인공 신경망"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def step(x) :\n",
    "    y = np.where(x>0,1,-1)\n",
    "    return y.astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.arange(-5.,5.,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = step(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0xc488d68>"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD8CAYAAABzTgP2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n0isAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc5addd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Series(y, index=x).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x) :\n",
    "    return 1 / (1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00669285,  0.00739154,  0.00816257,  0.0090133 ,  0.0099518 ,\n",
       "        0.01098694,  0.01212843,  0.01338692,  0.01477403,  0.0163025 ,\n",
       "        0.01798621,  0.01984031,  0.02188127,  0.02412702,  0.02659699,\n",
       "        0.02931223,  0.03229546,  0.03557119,  0.03916572,  0.04310725,\n",
       "        0.04742587,  0.05215356,  0.05732418,  0.06297336,  0.06913842,\n",
       "        0.07585818,  0.0831727 ,  0.09112296,  0.09975049,  0.10909682,\n",
       "        0.11920292,  0.13010847,  0.14185106,  0.15446527,  0.16798161,\n",
       "        0.18242552,  0.19781611,  0.21416502,  0.23147522,  0.24973989,\n",
       "        0.26894142,  0.2890505 ,  0.31002552,  0.33181223,  0.35434369,\n",
       "        0.37754067,  0.40131234,  0.42555748,  0.450166  ,  0.47502081,\n",
       "        0.5       ,  0.52497919,  0.549834  ,  0.57444252,  0.59868766,\n",
       "        0.62245933,  0.64565631,  0.66818777,  0.68997448,  0.7109495 ,\n",
       "        0.73105858,  0.75026011,  0.76852478,  0.78583498,  0.80218389,\n",
       "        0.81757448,  0.83201839,  0.84553473,  0.85814894,  0.86989153,\n",
       "        0.88079708,  0.89090318,  0.90024951,  0.90887704,  0.9168273 ,\n",
       "        0.92414182,  0.93086158,  0.93702664,  0.94267582,  0.94784644,\n",
       "        0.95257413,  0.95689275,  0.96083428,  0.96442881,  0.96770454,\n",
       "        0.97068777,  0.97340301,  0.97587298,  0.97811873,  0.98015969,\n",
       "        0.98201379,  0.9836975 ,  0.98522597,  0.98661308,  0.98787157,\n",
       "        0.98901306,  0.9900482 ,  0.9909867 ,  0.99183743,  0.99260846])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0xc632390>"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRCQEqdxFRELQ/wfofIkADZF/agAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc649860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Series(y, index=x).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ReLU(x):\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,\n",
       "        0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,\n",
       "        0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,\n",
       "        0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,\n",
       "        0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0.1,  0.2,  0.3,  0.4,\n",
       "        0.5,  0.6,  0.7,  0.8,  0.9,  1. ,  1.1,  1.2,  1.3,  1.4,  1.5,\n",
       "        1.6,  1.7,  1.8,  1.9,  2. ,  2.1,  2.2,  2.3,  2.4,  2.5,  2.6,\n",
       "        2.7,  2.8,  2.9,  3. ,  3.1,  3.2,  3.3,  3.4,  3.5,  3.6,  3.7,\n",
       "        3.8,  3.9,  4. ,  4.1,  4.2,  4.3,  4.4,  4.5,  4.6,  4.7,  4.8,\n",
       "        4.9])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = ReLU(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0xc64fba8>"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc67d6a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Series(y, index=x).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 신경망 순전파"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.array([1.0,0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W1 = np.array([[0.1,0.3,0.5],[0.2,0.4,0.6]])\n",
    "b1 = np.array([0.1,0.2,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z1 = np.dot(x,W1)+b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a1 = sigmoid(z1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.57444252,  0.66818777,  0.75026011])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W2 = np.array([[0.1,0.4], [0.2,0.5], [0.3,0.6]])\n",
    "b2 = np.array([0.1,0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z2 = np.dot(a1,W2) + b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a2 = sigmoid(z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W3 = np.array([0.1,0.3])\n",
    "b3 = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z3 = np.dot(a2,W3) + b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a3 = sigmoid(z3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = a3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.59722796177405213"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 여기에서는 분류형으로 사용하였다. (회귀로 접근)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 if y > 0.5 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 손실함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def 평균제곱오차(y,y_pred) :\n",
    "    return np.mean((y-y_pred)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.array([0,0,1,0,0,0,0,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred1 = np.array([0.1,0.05,0.6,0.0,0.05,0.1,0.0, 0.1,0.0,0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred1.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred2 = np.array([\n",
    "    0.1, 0.05, 0.1, 0.0, 0.05,\n",
    "    0.1, 0.0, 0.6, 0.0, 0.0\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred2.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = 평균제곱오차"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.019500000000000007"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(y, y_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11950000000000001"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(y, y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_entropy_error(y,y_pred) :\n",
    "    delta = 1e-7\n",
    "    return -np.sum(y*np.log(y_pred+delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = cross_entropy_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51082545709933802"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(y, y_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3025840929945458"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(y, y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iris = pd.read_csv('../data/iris.data', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = iris[4]\n",
    "X = iris.iloc[:,0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.38870874,  1.37549719, -2.08149935, -0.96033949],\n",
       "       [ 0.44214868, -1.61466519,  0.61811179, -1.39711532],\n",
       "       [-1.53026422, -1.08312912,  2.03766881,  2.24974372]])"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91111111111111109"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.pkl', 'wb') as file :\n",
    "    pickle.dump(model,file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('model.pkl','rb') as file :\n",
    "    model = pickle.load(file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91111111111111109"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 4)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    6.6\n",
       "1    3.0\n",
       "2    4.4\n",
       "3    1.4\n",
       "Name: 75, dtype: float64"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\python-opensource-for-deep-learning\\\\notebook\\\\predict.py'"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import predict\n",
    "predict.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'predict'"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict.__name__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting mnist/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting mnist/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting mnist/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('mnist/',one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = mnist.train.images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.reshape(x, (28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.misc import imsave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imsave('mnist_sample.png',x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망 모델 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 28*28])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.zeros([28*28, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# softmax는 다중분류의 출력값을 조정하는데 필수적입니다>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(a):\n",
    "    c = np.max(a)\n",
    "    ea = np.exp(a-c)\n",
    "    return ea / np.sum(ea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.array([0.3,2.9,4.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.1999999999999993"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = softmax(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01821127,  0.24519181,  0.73659691])"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 예측값 y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = tf.nn.softmax(tf.matmul(x,W)  + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정답 y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_ = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "손실함수\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(\n",
    "    -tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "경사하강법 (Gradient Descent)로 손실함수 최소화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "계산그래프 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "세션 = tf.InteractiveSession()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1000번 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for _ in range(1000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    세션.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "정확도 = tf.reduce_mean(tf.cast(y_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9178\n"
     ]
    }
   ],
   "source": [
    "print(세션.run(\n",
    "    정확도, \n",
    "    feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=uint8)"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(60000, 28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.reshape(10000,28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자료형이 중요하다고 함\n",
    "\n",
    "모두 float32형태로 바꿔줘야 한다고 한다. 그런데 왜 float64를 안쓸까 공간효율?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = X_test.astype('float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(255.0, 0.0)"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "( X_train.max() , X_train.min() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=uint8)"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.0)"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "( X_train.max() , X_train.min() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_train = np_utils.to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "층 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers.core import Dense, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1 = Dense(10, input_shape=(28*28,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(layer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 컴파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "딥러닝에선 cross validation을 할 경우는 조심해야 한다. 왜냐하면 트레이닝 시간이 오래 걸리기 때문에 \n",
    "그리고 에폭과 배치사이즈는 [여기](http://www.crowdlab.co.kr/questions/351/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%85%8B%EC%97%90-%EA%B4%80%EB%A0%A8%EB%90%9C-%ED%95%99%EC%8A%B5%EA%B3%BC%EC%A0%95%EC%9D%98-batch-size%EB%82%98-epoch%EA%B0%99%EC%9D%80-%EC%88%98%EC%B9%98%EC%9D%98-%EC%A1%B0%EC%A0%88%EC%97%90%EB%8C%80%ED%95%9C-%EA%B2%B0%EA%B3%BC%EA%B0%80-%EC%96%B4%EB%96%A0%ED%95%9C%EC%A7%80-%EA%B6%81%EA%B8%88%ED%95%A9%EB%8B%88%EB%8B%A4;jsessionid=15C08782360F21E7516E6E6B842DA832)에서 설명을 잘해놨다. \n",
    "\n",
    "가중치 갱신을 하기전에 batch_size는 실제로 X에 샘플을 몇번을 돌려보고 할것인가\n",
    "그리고 애폭은 전체 데이터를 몇번 돌려봤냐의 카운트\n",
    "\n",
    "즉 max_iter * batch size / whole data count 하면 됨\n",
    "\n",
    "예시\n",
    "=======================\n",
    "참조 : http://fbsight.com/t/epoch-batch/84691/2\n",
    "\n",
    "가진 데이터셋에 총 10,000개의 샘플이 있을 때\n",
    "\n",
    "몇개의 샘플을 가져와서 웨이트를 업데이트 시킬 것인가? = 배치사이즈\n",
    "배치사이즈1일 경우 = SGD\n",
    "- 1만개를 1개씩 쪼갬\n",
    "- 1개 가져와서 포워드 한번 백워드 한번 후 업데이트 (iteration step)\n",
    "- 그다음 하나 가져와서 포워드 한번 백워드 한번 후 업데이트 (iteration step)\n",
    "반복...\n",
    "\n",
    "배치사이즈 10일 경우\n",
    "- 1만개의 데이터셋을 10개씩(배치) 쪼갬\n",
    "- 하나의 배치에는 10개의 데이터가 존재\n",
    "- 하나의 배치 가져와서 포워드 한번 백워드 한번 후 업데이트 (iteration step)\n",
    "- 그다음 배치 가져와서 포워드 한번 백워드 한번 후 업데이트 (iteration step)\n",
    "\n",
    "모든 배치를 다 돈다면?\n",
    "모든 데이터셋에 대해 한번씩 다 돌아본게 됨으로 epoch 1회라고 함\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/200\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 1.3845 - acc: 0.6726 - val_loss: 0.8966 - val_acc: 0.8253\n",
      "Epoch 2/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.7936 - acc: 0.8276 - val_loss: 0.6592 - val_acc: 0.8553\n",
      "Epoch 3/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.6443 - acc: 0.8496 - val_loss: 0.5642 - val_acc: 0.8677\n",
      "Epoch 4/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.5721 - acc: 0.8607 - val_loss: 0.5116 - val_acc: 0.8772\n",
      "Epoch 5/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.5282 - acc: 0.8679 - val_loss: 0.4776 - val_acc: 0.8809\n",
      "Epoch 6/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.4980 - acc: 0.8728 - val_loss: 0.4532 - val_acc: 0.8853\n",
      "Epoch 7/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.4755 - acc: 0.8769 - val_loss: 0.4351 - val_acc: 0.8892\n",
      "Epoch 8/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.4581 - acc: 0.8804 - val_loss: 0.4209 - val_acc: 0.8916\n",
      "Epoch 9/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.4441 - acc: 0.8828 - val_loss: 0.4091 - val_acc: 0.8943\n",
      "Epoch 10/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.4325 - acc: 0.8848 - val_loss: 0.3995 - val_acc: 0.8964\n",
      "Epoch 11/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.4227 - acc: 0.8868 - val_loss: 0.3913 - val_acc: 0.8985\n",
      "Epoch 12/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.4142 - acc: 0.8891 - val_loss: 0.3843 - val_acc: 0.8997\n",
      "Epoch 13/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.4068 - acc: 0.8906 - val_loss: 0.3781 - val_acc: 0.9011\n",
      "Epoch 14/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.4003 - acc: 0.8920 - val_loss: 0.3728 - val_acc: 0.9020\n",
      "Epoch 15/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.3945 - acc: 0.8934 - val_loss: 0.3679 - val_acc: 0.9034\n",
      "Epoch 16/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.3893 - acc: 0.8941 - val_loss: 0.3636 - val_acc: 0.9038\n",
      "Epoch 17/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.3846 - acc: 0.8954 - val_loss: 0.3596 - val_acc: 0.9047\n",
      "Epoch 18/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.3803 - acc: 0.8961 - val_loss: 0.3561 - val_acc: 0.9051\n",
      "Epoch 19/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.3763 - acc: 0.8968 - val_loss: 0.3528 - val_acc: 0.9055\n",
      "Epoch 20/200\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.3726 - acc: 0.8974 - val_loss: 0.3498 - val_acc: 0.9058\n",
      "Epoch 21/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.3693 - acc: 0.8985 - val_loss: 0.3471 - val_acc: 0.9064\n",
      "Epoch 22/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.3662 - acc: 0.8990 - val_loss: 0.3444 - val_acc: 0.9067\n",
      "Epoch 23/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.3633 - acc: 0.8994 - val_loss: 0.3420 - val_acc: 0.9074\n",
      "Epoch 24/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.3605 - acc: 0.9000 - val_loss: 0.3398 - val_acc: 0.9073\n",
      "Epoch 25/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.3580 - acc: 0.9008 - val_loss: 0.3376 - val_acc: 0.9083\n",
      "Epoch 26/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.3556 - acc: 0.9016 - val_loss: 0.3357 - val_acc: 0.9076\n",
      "Epoch 27/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.3533 - acc: 0.9021 - val_loss: 0.3340 - val_acc: 0.9075\n",
      "Epoch 28/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.3512 - acc: 0.9022 - val_loss: 0.3321 - val_acc: 0.9093\n",
      "Epoch 29/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.3491 - acc: 0.9027 - val_loss: 0.3305 - val_acc: 0.9086\n",
      "Epoch 30/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.3472 - acc: 0.9033 - val_loss: 0.3289 - val_acc: 0.9095\n",
      "Epoch 31/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.3453 - acc: 0.9039 - val_loss: 0.3274 - val_acc: 0.9098\n",
      "Epoch 32/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.3436 - acc: 0.9043 - val_loss: 0.3259 - val_acc: 0.9103\n",
      "Epoch 33/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.3419 - acc: 0.9050 - val_loss: 0.3246 - val_acc: 0.9109\n",
      "Epoch 34/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.3404 - acc: 0.9053 - val_loss: 0.3232 - val_acc: 0.9117\n",
      "Epoch 35/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.3388 - acc: 0.9058 - val_loss: 0.3221 - val_acc: 0.9115\n",
      "Epoch 36/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.3374 - acc: 0.9060 - val_loss: 0.3208 - val_acc: 0.9120\n",
      "Epoch 37/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.3360 - acc: 0.9065 - val_loss: 0.3197 - val_acc: 0.9121\n",
      "Epoch 38/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.3346 - acc: 0.9069 - val_loss: 0.3186 - val_acc: 0.9121\n",
      "Epoch 39/200\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.3333 - acc: 0.9071 - val_loss: 0.3176 - val_acc: 0.9122\n",
      "Epoch 40/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.3321 - acc: 0.9079 - val_loss: 0.3166 - val_acc: 0.9127\n",
      "Epoch 41/200\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.3309 - acc: 0.9081 - val_loss: 0.3156 - val_acc: 0.9124\n",
      "Epoch 42/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.3297 - acc: 0.9083 - val_loss: 0.3147 - val_acc: 0.9134\n",
      "Epoch 43/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.3286 - acc: 0.9086 - val_loss: 0.3139 - val_acc: 0.9136\n",
      "Epoch 44/200\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.3275 - acc: 0.9094 - val_loss: 0.3130 - val_acc: 0.9135\n",
      "Epoch 45/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.3265 - acc: 0.9095 - val_loss: 0.3122 - val_acc: 0.9143\n",
      "Epoch 46/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.3255 - acc: 0.9097 - val_loss: 0.3112 - val_acc: 0.9143\n",
      "Epoch 47/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.3245 - acc: 0.9101 - val_loss: 0.3105 - val_acc: 0.9142\n",
      "Epoch 48/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.3235 - acc: 0.9102 - val_loss: 0.3098 - val_acc: 0.9148\n",
      "Epoch 49/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.3226 - acc: 0.9105 - val_loss: 0.3090 - val_acc: 0.9145\n",
      "Epoch 50/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.3217 - acc: 0.9109 - val_loss: 0.3083 - val_acc: 0.9147\n",
      "Epoch 51/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.3208 - acc: 0.9109 - val_loss: 0.3076 - val_acc: 0.9149\n",
      "Epoch 52/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.3200 - acc: 0.9114 - val_loss: 0.3072 - val_acc: 0.9147\n",
      "Epoch 53/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.3192 - acc: 0.9113 - val_loss: 0.3063 - val_acc: 0.9149\n",
      "Epoch 54/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.3184 - acc: 0.9117 - val_loss: 0.3058 - val_acc: 0.9153\n",
      "Epoch 55/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.3176 - acc: 0.9121 - val_loss: 0.3052 - val_acc: 0.9151\n",
      "Epoch 56/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.3169 - acc: 0.9122 - val_loss: 0.3045 - val_acc: 0.9150\n",
      "Epoch 57/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.3161 - acc: 0.9124 - val_loss: 0.3040 - val_acc: 0.9153\n",
      "Epoch 58/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.3154 - acc: 0.9125 - val_loss: 0.3035 - val_acc: 0.9153\n",
      "Epoch 59/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.3148 - acc: 0.9128 - val_loss: 0.3028 - val_acc: 0.9157\n",
      "Epoch 60/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.3141 - acc: 0.9130 - val_loss: 0.3023 - val_acc: 0.9158\n",
      "Epoch 61/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.3134 - acc: 0.9130 - val_loss: 0.3018 - val_acc: 0.9159\n",
      "Epoch 62/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.3127 - acc: 0.9133 - val_loss: 0.3013 - val_acc: 0.9160\n",
      "Epoch 63/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.3121 - acc: 0.9135 - val_loss: 0.3009 - val_acc: 0.9163\n",
      "Epoch 64/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.3115 - acc: 0.9134 - val_loss: 0.3004 - val_acc: 0.9157\n",
      "Epoch 65/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.3109 - acc: 0.9137 - val_loss: 0.2999 - val_acc: 0.9163\n",
      "Epoch 66/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.3103 - acc: 0.9141 - val_loss: 0.2995 - val_acc: 0.9164\n",
      "Epoch 67/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.3097 - acc: 0.9144 - val_loss: 0.2991 - val_acc: 0.9166\n",
      "Epoch 68/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.3091 - acc: 0.9142 - val_loss: 0.2986 - val_acc: 0.9162\n",
      "Epoch 69/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.3086 - acc: 0.9148 - val_loss: 0.2982 - val_acc: 0.9165\n",
      "Epoch 70/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.3080 - acc: 0.9148 - val_loss: 0.2978 - val_acc: 0.9165\n",
      "Epoch 71/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.3075 - acc: 0.9149 - val_loss: 0.2973 - val_acc: 0.9170\n",
      "Epoch 72/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.3070 - acc: 0.9149 - val_loss: 0.2969 - val_acc: 0.9172\n",
      "Epoch 73/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.3065 - acc: 0.9151 - val_loss: 0.2966 - val_acc: 0.9171\n",
      "Epoch 74/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.3060 - acc: 0.9151 - val_loss: 0.2962 - val_acc: 0.9170\n",
      "Epoch 75/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.3055 - acc: 0.9151 - val_loss: 0.2958 - val_acc: 0.9172\n",
      "Epoch 76/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.3050 - acc: 0.9153 - val_loss: 0.2954 - val_acc: 0.9175\n",
      "Epoch 77/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.3046 - acc: 0.9155 - val_loss: 0.2950 - val_acc: 0.9177\n",
      "Epoch 78/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.3041 - acc: 0.9157 - val_loss: 0.2946 - val_acc: 0.9179\n",
      "Epoch 79/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.3036 - acc: 0.9160 - val_loss: 0.2944 - val_acc: 0.9179\n",
      "Epoch 80/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.3032 - acc: 0.9160 - val_loss: 0.2940 - val_acc: 0.9180\n",
      "Epoch 81/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.3027 - acc: 0.9159 - val_loss: 0.2938 - val_acc: 0.9175\n",
      "Epoch 82/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.3023 - acc: 0.9159 - val_loss: 0.2934 - val_acc: 0.9177\n",
      "Epoch 83/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.3019 - acc: 0.9164 - val_loss: 0.2930 - val_acc: 0.9184\n",
      "Epoch 84/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.3015 - acc: 0.9161 - val_loss: 0.2927 - val_acc: 0.9185\n",
      "Epoch 85/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.3011 - acc: 0.9163 - val_loss: 0.2925 - val_acc: 0.9179\n",
      "Epoch 86/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.3007 - acc: 0.9162 - val_loss: 0.2922 - val_acc: 0.9177\n",
      "Epoch 87/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.3003 - acc: 0.9165 - val_loss: 0.2919 - val_acc: 0.9180\n",
      "Epoch 88/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2999 - acc: 0.9165 - val_loss: 0.2916 - val_acc: 0.9187\n",
      "Epoch 89/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2995 - acc: 0.9167 - val_loss: 0.2913 - val_acc: 0.9185\n",
      "Epoch 90/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2991 - acc: 0.9170 - val_loss: 0.2910 - val_acc: 0.9182\n",
      "Epoch 91/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2988 - acc: 0.9166 - val_loss: 0.2908 - val_acc: 0.9188\n",
      "Epoch 92/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2984 - acc: 0.9168 - val_loss: 0.2906 - val_acc: 0.9185\n",
      "Epoch 93/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2980 - acc: 0.9170 - val_loss: 0.2903 - val_acc: 0.9186\n",
      "Epoch 94/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2977 - acc: 0.9172 - val_loss: 0.2900 - val_acc: 0.9188\n",
      "Epoch 95/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2973 - acc: 0.9174 - val_loss: 0.2898 - val_acc: 0.9187\n",
      "Epoch 96/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2970 - acc: 0.9173 - val_loss: 0.2895 - val_acc: 0.9191\n",
      "Epoch 97/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2967 - acc: 0.9174 - val_loss: 0.2893 - val_acc: 0.9193\n",
      "Epoch 98/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2963 - acc: 0.9175 - val_loss: 0.2891 - val_acc: 0.9192\n",
      "Epoch 99/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2960 - acc: 0.9174 - val_loss: 0.2888 - val_acc: 0.9192\n",
      "Epoch 100/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2957 - acc: 0.9179 - val_loss: 0.2886 - val_acc: 0.9195\n",
      "Epoch 101/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2954 - acc: 0.9180 - val_loss: 0.2884 - val_acc: 0.9192\n",
      "Epoch 102/200\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.2950 - acc: 0.9180 - val_loss: 0.2882 - val_acc: 0.9193\n",
      "Epoch 103/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2947 - acc: 0.9181 - val_loss: 0.2879 - val_acc: 0.9198\n",
      "Epoch 104/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2944 - acc: 0.9180 - val_loss: 0.2878 - val_acc: 0.9196\n",
      "Epoch 105/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2942 - acc: 0.9181 - val_loss: 0.2875 - val_acc: 0.9202\n",
      "Epoch 106/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2938 - acc: 0.9182 - val_loss: 0.2873 - val_acc: 0.9202\n",
      "Epoch 107/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2936 - acc: 0.9182 - val_loss: 0.2870 - val_acc: 0.9197\n",
      "Epoch 108/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2932 - acc: 0.9184 - val_loss: 0.2869 - val_acc: 0.9204\n",
      "Epoch 109/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2930 - acc: 0.9185 - val_loss: 0.2867 - val_acc: 0.9197\n",
      "Epoch 110/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2927 - acc: 0.9187 - val_loss: 0.2865 - val_acc: 0.9203\n",
      "Epoch 111/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2924 - acc: 0.9186 - val_loss: 0.2863 - val_acc: 0.9199\n",
      "Epoch 112/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2921 - acc: 0.9190 - val_loss: 0.2861 - val_acc: 0.9197\n",
      "Epoch 113/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2919 - acc: 0.9190 - val_loss: 0.2859 - val_acc: 0.9207\n",
      "Epoch 114/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2916 - acc: 0.9191 - val_loss: 0.2857 - val_acc: 0.9209\n",
      "Epoch 115/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2913 - acc: 0.9191 - val_loss: 0.2855 - val_acc: 0.9209\n",
      "Epoch 116/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2911 - acc: 0.9190 - val_loss: 0.2853 - val_acc: 0.9203\n",
      "Epoch 117/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2908 - acc: 0.9190 - val_loss: 0.2852 - val_acc: 0.9207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2906 - acc: 0.9192 - val_loss: 0.2849 - val_acc: 0.9203\n",
      "Epoch 119/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2903 - acc: 0.9194 - val_loss: 0.2848 - val_acc: 0.9207\n",
      "Epoch 120/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2900 - acc: 0.9193 - val_loss: 0.2846 - val_acc: 0.9207\n",
      "Epoch 121/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2898 - acc: 0.9192 - val_loss: 0.2844 - val_acc: 0.9212\n",
      "Epoch 122/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2896 - acc: 0.9193 - val_loss: 0.2843 - val_acc: 0.9212\n",
      "Epoch 123/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2893 - acc: 0.9197 - val_loss: 0.2842 - val_acc: 0.9212\n",
      "Epoch 124/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2891 - acc: 0.9196 - val_loss: 0.2840 - val_acc: 0.9207\n",
      "Epoch 125/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2888 - acc: 0.9197 - val_loss: 0.2839 - val_acc: 0.9207\n",
      "Epoch 126/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2886 - acc: 0.9198 - val_loss: 0.2836 - val_acc: 0.9208\n",
      "Epoch 127/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2884 - acc: 0.9199 - val_loss: 0.2835 - val_acc: 0.9206\n",
      "Epoch 128/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2882 - acc: 0.9199 - val_loss: 0.2833 - val_acc: 0.9206\n",
      "Epoch 129/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2880 - acc: 0.9196 - val_loss: 0.2832 - val_acc: 0.9207\n",
      "Epoch 130/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2877 - acc: 0.9198 - val_loss: 0.2830 - val_acc: 0.9209\n",
      "Epoch 131/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2875 - acc: 0.9203 - val_loss: 0.2829 - val_acc: 0.9212\n",
      "Epoch 132/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2873 - acc: 0.9202 - val_loss: 0.2827 - val_acc: 0.9210\n",
      "Epoch 133/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2870 - acc: 0.9200 - val_loss: 0.2827 - val_acc: 0.9208\n",
      "Epoch 134/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2869 - acc: 0.9199 - val_loss: 0.2825 - val_acc: 0.9212\n",
      "Epoch 135/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2866 - acc: 0.9203 - val_loss: 0.2823 - val_acc: 0.9213\n",
      "Epoch 136/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2864 - acc: 0.9204 - val_loss: 0.2822 - val_acc: 0.9210\n",
      "Epoch 137/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2862 - acc: 0.9204 - val_loss: 0.2820 - val_acc: 0.9214\n",
      "Epoch 138/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2860 - acc: 0.9204 - val_loss: 0.2819 - val_acc: 0.9215\n",
      "Epoch 139/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2858 - acc: 0.9204 - val_loss: 0.2818 - val_acc: 0.9212\n",
      "Epoch 140/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2856 - acc: 0.9204 - val_loss: 0.2815 - val_acc: 0.9215\n",
      "Epoch 141/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2854 - acc: 0.9208 - val_loss: 0.2814 - val_acc: 0.9217\n",
      "Epoch 142/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2852 - acc: 0.9204 - val_loss: 0.2813 - val_acc: 0.9216\n",
      "Epoch 143/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2850 - acc: 0.9207 - val_loss: 0.2812 - val_acc: 0.9217\n",
      "Epoch 144/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2848 - acc: 0.9208 - val_loss: 0.2810 - val_acc: 0.9217\n",
      "Epoch 145/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2846 - acc: 0.9211 - val_loss: 0.2809 - val_acc: 0.9215\n",
      "Epoch 146/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2845 - acc: 0.9207 - val_loss: 0.2808 - val_acc: 0.9215\n",
      "Epoch 147/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2843 - acc: 0.9209 - val_loss: 0.2807 - val_acc: 0.9217\n",
      "Epoch 148/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2841 - acc: 0.9209 - val_loss: 0.2806 - val_acc: 0.9219\n",
      "Epoch 149/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2839 - acc: 0.9209 - val_loss: 0.2805 - val_acc: 0.9215\n",
      "Epoch 150/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2837 - acc: 0.9210 - val_loss: 0.2803 - val_acc: 0.9215\n",
      "Epoch 151/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2835 - acc: 0.9213 - val_loss: 0.2804 - val_acc: 0.9217\n",
      "Epoch 152/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2834 - acc: 0.9213 - val_loss: 0.2801 - val_acc: 0.9217\n",
      "Epoch 153/200\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.2832 - acc: 0.9208 - val_loss: 0.2800 - val_acc: 0.9216\n",
      "Epoch 154/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2830 - acc: 0.9212 - val_loss: 0.2799 - val_acc: 0.9219\n",
      "Epoch 155/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2828 - acc: 0.9214 - val_loss: 0.2798 - val_acc: 0.9221\n",
      "Epoch 156/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2826 - acc: 0.9212 - val_loss: 0.2797 - val_acc: 0.9217\n",
      "Epoch 157/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2825 - acc: 0.9213 - val_loss: 0.2795 - val_acc: 0.9222\n",
      "Epoch 158/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2823 - acc: 0.9214 - val_loss: 0.2795 - val_acc: 0.9219\n",
      "Epoch 159/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2821 - acc: 0.9216 - val_loss: 0.2794 - val_acc: 0.9222\n",
      "Epoch 160/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2820 - acc: 0.9215 - val_loss: 0.2792 - val_acc: 0.9219\n",
      "Epoch 161/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2818 - acc: 0.9216 - val_loss: 0.2792 - val_acc: 0.9217\n",
      "Epoch 162/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2817 - acc: 0.9217 - val_loss: 0.2790 - val_acc: 0.9219\n",
      "Epoch 163/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2815 - acc: 0.9214 - val_loss: 0.2789 - val_acc: 0.9217\n",
      "Epoch 164/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2813 - acc: 0.9218 - val_loss: 0.2788 - val_acc: 0.9219\n",
      "Epoch 165/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2812 - acc: 0.9219 - val_loss: 0.2787 - val_acc: 0.9223\n",
      "Epoch 166/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2810 - acc: 0.9217 - val_loss: 0.2786 - val_acc: 0.9222\n",
      "Epoch 167/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2809 - acc: 0.9218 - val_loss: 0.2785 - val_acc: 0.9218\n",
      "Epoch 168/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2807 - acc: 0.9216 - val_loss: 0.2785 - val_acc: 0.9218\n",
      "Epoch 169/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2805 - acc: 0.9217 - val_loss: 0.2784 - val_acc: 0.9217\n",
      "Epoch 170/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2804 - acc: 0.9219 - val_loss: 0.2782 - val_acc: 0.9222\n",
      "Epoch 171/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2802 - acc: 0.9219 - val_loss: 0.2782 - val_acc: 0.9217\n",
      "Epoch 172/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2801 - acc: 0.9216 - val_loss: 0.2780 - val_acc: 0.9220\n",
      "Epoch 173/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2799 - acc: 0.9220 - val_loss: 0.2780 - val_acc: 0.9222\n",
      "Epoch 174/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2798 - acc: 0.9219 - val_loss: 0.2779 - val_acc: 0.9223\n",
      "Epoch 175/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2796 - acc: 0.9218 - val_loss: 0.2778 - val_acc: 0.9224\n",
      "Epoch 176/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2795 - acc: 0.9219 - val_loss: 0.2777 - val_acc: 0.9222\n",
      "Epoch 177/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2794 - acc: 0.9220 - val_loss: 0.2776 - val_acc: 0.9220\n",
      "Epoch 178/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2792 - acc: 0.9219 - val_loss: 0.2775 - val_acc: 0.9219\n",
      "Epoch 179/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2790 - acc: 0.9221 - val_loss: 0.2775 - val_acc: 0.9219\n",
      "Epoch 180/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2789 - acc: 0.9221 - val_loss: 0.2774 - val_acc: 0.9227\n",
      "Epoch 181/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2788 - acc: 0.9221 - val_loss: 0.2772 - val_acc: 0.9225\n",
      "Epoch 182/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2787 - acc: 0.9224 - val_loss: 0.2772 - val_acc: 0.9226\n",
      "Epoch 183/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2785 - acc: 0.9223 - val_loss: 0.2771 - val_acc: 0.9221\n",
      "Epoch 184/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2784 - acc: 0.9222 - val_loss: 0.2770 - val_acc: 0.9224\n",
      "Epoch 185/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2782 - acc: 0.9224 - val_loss: 0.2770 - val_acc: 0.9221\n",
      "Epoch 186/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2781 - acc: 0.9221 - val_loss: 0.2768 - val_acc: 0.9222\n",
      "Epoch 187/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2780 - acc: 0.9224 - val_loss: 0.2767 - val_acc: 0.9220\n",
      "Epoch 188/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2778 - acc: 0.9226 - val_loss: 0.2767 - val_acc: 0.9229\n",
      "Epoch 189/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2777 - acc: 0.9220 - val_loss: 0.2766 - val_acc: 0.9228\n",
      "Epoch 190/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2776 - acc: 0.9225 - val_loss: 0.2765 - val_acc: 0.9228\n",
      "Epoch 191/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2774 - acc: 0.9228 - val_loss: 0.2765 - val_acc: 0.9225\n",
      "Epoch 192/200\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.2773 - acc: 0.9226 - val_loss: 0.2764 - val_acc: 0.9223\n",
      "Epoch 193/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2772 - acc: 0.9228 - val_loss: 0.2763 - val_acc: 0.9229\n",
      "Epoch 194/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2771 - acc: 0.9227 - val_loss: 0.2762 - val_acc: 0.9227\n",
      "Epoch 195/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2769 - acc: 0.9225 - val_loss: 0.2761 - val_acc: 0.9225\n",
      "Epoch 196/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2768 - acc: 0.9227 - val_loss: 0.2761 - val_acc: 0.9224\n",
      "Epoch 197/200\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.2766 - acc: 0.9226 - val_loss: 0.2760 - val_acc: 0.9225\n",
      "Epoch 198/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2766 - acc: 0.9227 - val_loss: 0.2759 - val_acc: 0.9228\n",
      "Epoch 199/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2764 - acc: 0.9227 - val_loss: 0.2759 - val_acc: 0.9229\n",
      "Epoch 200/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2763 - acc: 0.9226 - val_loss: 0.2758 - val_acc: 0.9223\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train, batch_size=128, epochs=200, validation_split=0.2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results = DataFrame(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x12f24668>"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x14201c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_results[['acc','val_acc']].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 위의 결과\n",
    "위의 결과를 보면 과소적합 과적합 둘다 안일어나고 빠르게 최적화/일반화가 이뤄진걸 볼 수 있따."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "은닉층(Hidden Layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(128, input_shape=(28*28,), activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Dense(128, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Dense(128, activation='relu')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "컴파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 컴파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='categorical_crossentropy', optimizer='sgd',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 1.3911 - acc: 0.6249\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.4784 - acc: 0.8691\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.3643 - acc: 0.8957\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.3177 - acc: 0.9078\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.2883 - acc: 0.9156\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.2657 - acc: 0.9222\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.2475 - acc: 0.9275\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.2321 - acc: 0.9326\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.2185 - acc: 0.9368\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.2065 - acc: 0.9400\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.1959 - acc: 0.9429\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.1864 - acc: 0.9457\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.1778 - acc: 0.9479\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.1695 - acc: 0.9508\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.1620 - acc: 0.9529\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.1556 - acc: 0.9550\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.1494 - acc: 0.9566\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.1437 - acc: 0.9581\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.1382 - acc: 0.9601\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 1s 23us/step - loss: 0.1326 - acc: 0.9615\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, Y_train, batch_size=128, epochs=20,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 134,794\n",
      "Trainable params: 134,794\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
